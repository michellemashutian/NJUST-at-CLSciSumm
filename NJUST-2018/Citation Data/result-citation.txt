 In statistical methods, the most popular models are Hidden Markov Models (HMM) (Rabiner, 1989), Maximum Entropy Models (ME) (Chieu et al., 2002) and Conditional Random Fields (CRF) (Lafferty et al., 2001). 	  Results_Citation 
 The latter is currently dominating in NER amongst which the most popular methods are decision tree (Sekine et al., 1998; Pailouras et al., 2000), Hidden Markov Model (Zhang et al., 2003; Zhao, 2004), maximum entropy (Chieu and Ng, 2002; Bender et al., 2003), and support vector machines (Isozaki and Kazawa, 2002; Takeuchi and Collier, 2002; Mayfield, 2003).From the linguistic perspective, NIL expres sions are rather different from named entities in nature. 	  Results_Citation 
 modelâ€™s conditional probability is defined as Another model is Maximum Entropy (Zhao Jian 2005, Hai Leong Chieu 2002). 	 Results_Citation 
 Such global features enhance the performance of NER (Chieu and Ng, 2002b). 	  Results_Citation 
 To develop UWI, there are three approaches	  Results_Citation 
 Additional German tags are obtained using the RFTagger 2 toolkit, which annotates text with fine-grained part-of-speech tags (Schmid and Laws, 2008) with a vocabulary of more than 700 tags containing rich morpho-syntactic information (gender, number, case, tense, etc.). 	  ['Implication_Citation','Results_Citation'] 
 For German, finally, we see the greatest improvement with k = 3 tional words that are not found in the training corpus and additional tags for words that do occur in the training data (Schmid and Laws, 2008). 	  Results_Citation 
 For German we used morphologically rich tags from RFTagger (Schmid and Laws, 2008), that contains morphological information such as case, number, and gender for nouns and tense for verbs. 	  Results_Citation 
 The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information.While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus. 	  Results_Citation 
 The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information. 	  Results_Citation 
 The results presented here were achieved using the RFTagger (Schmid and Laws, 2008) 	  ['Method_Citation','Results_Citation'] 
 One possible solution to the unobserved word-sequence problem is a pipeline system in which an initial model is in charge of token-segmentation, and the output of the initial model is fed as the input to a second stage parser.This is a popular approach in parsing systems for Arabic and Chinese (Jiang, Huang, and Liu 2009; Green and Manning 2010). 	  Results_Citation 
 Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al.(1999), Simaâ€™an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010). 	  Results_Citation 
 The best reported results for parsing Arabic when the gold word segmentation is not known, however, are obtained using a pipeline model in which a tagger and word-segmenter is applied prior to a manually state-split constituency parser, resulting in an F-score of 79% F1 (for sentences of up to 70 words) (Green and Manning 2010). 	  Results_Citation 
 Recently, Green and Manning (2010) analyzed the PATB for annotation consistency, and introduced an enhanced split-state constituency grammar, including labels for short idafa constructions and verbal or equational clauses. 	  ['Results_Citation','Method_Citation'] 
 For Arabic, we use the head-finding rules from Green and Manning (2010). 	  Results_Citation 
 We previously showed that the â€œKulickâ€ tag set is very effective for basic Arabic parsing (Green and Manning 2010). 	  Results_Citation 
 We previously showed that segmentation errors decrease Arabic parsing accuracy by about 2.0% F1 (Green and Manning 2010). 	  Results_Citation 
 Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010) 	  Results_Citation 
 Green and Manning (2010) discuss annotation consistency in the Penn Arabic Treebank (ATB) 	  Results_Citation 
 Measuring recall is tricky, even using the errors identified in Green and Manning (2010) as â€œgoldâ€ errors. 	  Results_Citation 
 Green and Manning (2010) obtain the opposite result in their Arabic parsing experiments, with the lattice parser underperforming the pipeline system by over 3 points (76.01 F1 vs 79.17 F1). 	  Results_Citation 
 Green and Manning (2010) find that using automatic tokenization provided by MADA (Habash et al., 2009) instead of gold tokenization results in a 1.92% F score drop in their constituent parsing work. 	  Results_Citation 
 That is, unless some new scheme for reducing excessive copying is introduced such as scucture-sharing of an unchanged shared-forest ([Kogure, 1990]). 	  Results_Citation 
 Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009). 	 Result_Citation 
 On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). 	 Result_Citation 
 Following Lee et al.(2010), we report the best and median settings of hyperparameters based on the F- score, in addition to inferred values. 	  Results_Citation 
 This property is not strictly true of linguistic data, but is a good approximation	  Results_Citation 
 Following Lee et al.(2010) we used only the training sections for each language. 	  Results_Citation 
 Given that close to 95% of the word occurrences in human labeled data are tagged with their most frequent part of speech (Lee et al., 2010) 	  Results_Citation 
 It is also interesting to compare the bigram PYP1HMM to the closely related model of Lee et al.(2010).That model incorrectly assumed independence of the conditional sampling distributions, resulting in a accuracy of 66.4% 	  Results_Citation 
 As they rely on the detection of high-density areas in a network of cooccurrences, (Véronis, 2003) and (Dorow and Widdows, 2003) are the closest methods to ours. 	  Results_Citation 
 First of all, it is really difficult to build a reliable and objective gold-standard given the fact that there is only 70% agreement between native speakers on this task (Sproat et al., 1996). 	  ['Implication_Citation','Results_Citation'] 
 According to Sproat et al. {1996) and Wu and Fung {1994), experiments show that only about 75% agreement between native speakers is to be expected on the "correct" segmentation, and the figure reduces as more people become involved. 	  Results_Citation 
 In Chinese text segmentation there are three basic approaches (Sproat et al. 1996)	  Results_Citation 
 As shown in Sproat et al.(1996), the rate of agreement between two human judges is less than 80%. 	  Results_Citation 
 Similarly, Sproat et al.(1996) also uses multiple human judges. 	  Results_Citation 
 Experiments have shown that there is only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al. 1996). 	  Results_Citation 
 In Japanese, around 95% word segmentation acÂ­ curacy is reported by using a word-based lanÂ­ guage model and the Viterbi-like dynamic programÂ­ ming procedures (Nagata, 1994; Yamamoto, 1996; Takeuchi and Matsumoto, 1997; Haruno and MatÂ­ sumoto, 1997).About the same accuracy is reported in Chinese by statistical methods (Sproat et al., 1996). 	  Results_Citation 
 There are multiple studies (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) showing that the agreement between two (untrained) native speakers is about upper to lower 	  Results_Citation 
 Chinese word segmentation is a well-known problem that has been studied extensively (Wu and Fung, 1994; Sproat et al., 1996; Luo and Roukos, 1996) and it is known that human agreement is relatively low. 	  Results_Citation 
 Experiments have shown only about 75% agreement among native speakers regarding the correct word segmentation (Sproat et al., 1996). 	  Results_Citation 
 Indeed, even native speakers can agree on word boundaries in modern Chinese only about 76% of the time (Sproat et al., 1996). 	  Results_Citation 
 No comparable figure has been reported for classical Chinese word segmentation, but this rate compares favorably with past attempts for modern Chinese, e.g., an average of 76% inter- human agreement rate in (Sproat et al., 1996). 	  Results_Citation 
 Word Segmentation accuracy is expressed in terms of recall and precision as is done for bracketing of partial parses [Nagata, 1994, Sproat et al., 1996). 	  ['Method_Citation','Results_Citation'] 
 in fact, Pedersen (2001) found that bigrams alone can be e.ective features for word sense disambiguation. 	  Results_Citation 
 Bigrams have recently been shown to be very successful features in supervised word sense disambiguation (Peder­sen, 2001). 	  Results_Citation 
 Bean and Riloff (2004) used high-precision hand-coded rules to identify coreferent mention pairs, which are then used to acquire role pairs that they refer to as Caseframe Network features.They use these features to improve coreference resolution for two domain-specific corpora involving terrorism and natural disasters. 	  ['Method_Citation','Results_Citation'] 
 CRF + Rule system represents a combination of CRF model and rule based model presented in Zhang et al.(2006). 	  ['Method_Citation','Results_Citation'] 
 After we get word-based segmentation result, we use it to revise the CRF tagging result similar to (Zhang et al., 2006). 	  Results_Citation 
 If the confidence of a character is lower than the threshold, the tag of that character will be adjusted to the tag assigned by the Maximum Probability Segmentation (R. Zhang et al., 2006). 	  Results_Citation 
 According to the results reported in (R. Zhang et al., 2006), CRF performs relatively better on Out-of-Vocabulary (OOV) words while Maximum Probability performs well on IV words, so a model combining the advantages of these two methods is appealing. 	  ['Method_Citation','Results_Citation'] 
 For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004). 	  ['Method_Citation','Results_Citation'] 
 The work closest to ours is the subjectivity word sense disambiguation method proposed in Akkaya et al. (2009), where on a set of 83 English words, an accuracy of 88% was observed; and the method proposed in Su and Markert (2009), where an accuracy of 84% was obtained on another dataset of 298 words. 	 Results_Citation 
 For example, Su and Markert (2009) make use of both Wordnet definitions and Wordnet relations and achieve an accuracy of 84.6% on all parts-of-speech. 	 ['Method_Citation', 'Results_Citation'] 
 Challenging non-concatenative morphological phenomena, such as circumfixion and root-and-pattern morphology, can be characterized by regular means (Beesley and Karttunen 2000, 2003). 	 Result Citation 
 However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy. 	  Results_Citation 
  dependency kernel Zhou et al.(2005) 	  Results_Citation 
 This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information. 	  Results_Citation 
 For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011). 	  ['Method_Citation','Results_Citation'] 
 The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually. 	  Results_Citation 
 Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction. 	  Results_Citation 
 Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively. 	  Results_Citation 
 Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list. 	  ['Results_Citation','Implication_Citation'] 
 While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE. 	  Results_Citation 
 Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). 	  ['Method_Citation','Results_Citation'] 
 Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features. 	  Results_Citation 
 Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs. 	  Results_Citation 
 However, detailed research (Zhou et al., 2005) shows that itâ€™s difficult to extract new effective features to further improve the extraction accuracy. 	  Results_Citation 
  dependency kernel Zhou et al.(2005) 	  Results_Citation 
 This is a lightweight model and generally does not attempt to exhaustively leverage all possible proven sources of useful features (Zhou et al., 2005) towards a higher absolute score, but rather to serve as a point of comparison to the models which rely on syntactic information. 	  Results_Citation 
 For the choice of features, we use the full set of features from Zhou et al.(2005) since it is reported to have a state-of-the-art performance (Sun et al., 2011). 	  ['Method_Citation','Results_Citation'] 
 The features used in Kambhatla (2004) and Zhou et al.(2005) have to be selected and carefully calibrated manually. 	  Results_Citation 
 Entity Attributes	  Results_Citation 
 Although in order to achieve the best performance, it is necessary to use a proper combination of these features (Zhou et al., 2005), in this paper, we will concentrate on how to better capture the syntactic features for relation extraction. 	  Results_Citation 
 Zhou et al (2005) further systematically explored diverse lexical, syntactic and semantic features through support vector machines and achieved F- measure of 68.1 and 55.5 on the 5 relation types and the 24 relation subtypes in the ACE RDC 2003 corpus respectively. 	  Results_Citation 
 Based on his work, Zhou et al (2005)further incorporated the base phrase chunking information and semi-automatically collected country name list and personal relative trigger word list. 	  ['Results_Citation','Implication_Citation'] 
 While syntactic features are known to improve the performance of supervised IE, at least using clean hand-labeled ACE data (Zhou et al., 2007; Zhou et al., 2005), we do not know whether syntactic features can improve the performance of unsupervised or distantly supervised IE. 	  Results_Citation 
 Recent work on relation extraction has shown that supervised machine learning coupled with intelligent feature engineering or kernel design provides state-of-the-art solutions to the problem (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). 	  ['Method_Citation','Results_Citation'] 
 Although a bit lower than Zhou et al.â€™s result of 55.5 (Zhou et al., 2005), we attribute the difference to our use of a different tokenizer, different parser, and having not used the semantic information features. 	  Results_Citation 
 Especially, although we did not concern the dependency tree and full parse tree information as other supervised methods (Miller et al., 2000; Culotta and Soresen, 2004; Kambhatla, 2004; Zhou et al., 2005), the incorporation of simple features, such as words and chunking information, still can provide complement information for capturing the characteristics of entity pairs. 	  Results_Citation 
 We adopted K = 3 topics, following the setting in (Zhao and Xing, 2006). 	  Results_Citation 
 The alignment results for both directions were refined with ‘GROW’ heuristics to yield high precision and high recall in accordance with previous work (Och and Ney, 2003; Zhao and Xing, 2006). 	  ['Method_Citation','Results_Citation'] 
 In (Zhao and Xing, 2006), three fairly sophisticated bayesian topical translation models, taking IBM Model 1 as a baseline model, were presented under the bilingual topic admixture model formalism.These models capture latent topics at the document level in order to reduce semantic ambiguity and improve translation coherence.The models proposed provide in some cases better word alignment and translation quality than HMM and IBM models on an EnglishChinese task. 	  ['Method_Citation','Results_Citation'] 
 Many verbs are listed in multiple classes, some of which have conflicting sets of syntactic frames.Dang ct al.(1998) showed that multiple listings could in some cases be interpreted as regular sense extensions, and defined intcrsectivc Levin classes, which are a more fine-grained, syntactically and semantically coherÂ­ ent refinement of basic Levin classes. 	  ['Results_Citation','Method_Citation'] 
 VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. 	  Results_Citation 
 Van Halteren et al.(1998) introduce a modi.ed version of voting called TagPair.Under this model, the conditional probability that the word sense is s given that classi.er ioutputs sand classi.er jout­puts s2, P(sls i(xd)=ss j(xd)=s2), is com­puted on development data, and the posterior prob­ability is estimated as N P(slx,d)eÆ(s,sAk(x,d))+Æ(s,sA j(x,d)) (7) k..j where sc;,j(xfd)=argmaxtP(tlsc;(xfd)fscj(xfd)).Each classi.er votes for its classi.cation and every pair of classi.ers votes for the sense that is most likely given the joint classi.cation.In the experi­ments presented in van Halteren et al.(1998), this method was the best performer among the presented methods. 	  Results_Citation 
 Thirdly, this approach is compatible with in­ corporating multiple components of the same type to improve performance (cf.(van Halteren et al., 1998) who found that combining the results of several part of speech taggers increased performance). 	  Results_Citation 
 Van Halteren et al.(1998) have generalized this approach for higher number of classifiers in their TotPrecision voting method.The vote of each classifier (parser) is weighted by their respective accuracy. 	  Results_Citation 
 First experiments (van Halteren, Zavrel, and Daelemans 1998; Brill and Wu 1998) demonstrated the basic validity of the approach for tagging, with the error rate of the best combiner being 19.1% lower than that of the best individual tagger (van Halteren, Zavrel, and Daelemans 1998).However, these experiments were restricted to a single language, a single tagset and, more importantly, a limited amount of training data for the combiners.This led us to perform further, more extensive, 1In previous work (van Halteren, Zavrel, and Daelemans 1998), we were unable to confirm the latter half of the hypothesis unequivocally. 	  Results_Citation 
 The most important result that has undergone a change between van Halteren, Zavrel, and Daelemans (1998) and our current experiments is the relative accuracy of TagPair and stacked systems such as MBL.Where TagPair used to be significantly better than MBL, the roles are now well reversed. 	  Results_Citation 
 In van Halteren, Zavrel, and Daelemans (1998) we used a straightforward im­ plementation of HMM&apos;s, which turned out to have the worst accuracy of the four competing methods. 	  Results_Citation 
 With LOB and a single 114K tune set (van Halteren, Zavrel, and Daelemans 1998), both MBL and Decision Trees degraded significantly when adding context, and MBL degraded when adding the word 	  Results_Citation 
 Nevertheless, recent results show that knowledge-poor methods perform with amazing acÂ­ curacy (cf.(Mitkov, 1998), (Kennedy and Boguraev, 1996) (Kameyama, 1997)). 	  Results_Citation 
 Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy.Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an F-measure of 82.8% for personal pronouns, respectively. 	  ['Aim_Citation','Results_Citation'] 
 The acquisition of exten sive linguistic and discourse knowledge necessaryfor resolving coreference is time consuming, diffi cult and error-prone.Neverthless, recent resultsshow that knowledge-poor, empirical methods per form with amazing accuracy on certain forms ofcoreference (cf.(Mitkov 1998) (Kennedy and Boguraev 1996) (Kameyama 1997)). 	  ['Method_Citation','Results_Citation'] 
 Mitkov showed that a salience-based approach can be applied across genres and without complex syntactic, semantic, and discourse analysis (Mitkov, 1998). 	  ['Method_Citation','Results_Citation'] 
 Mitkov (1998) obtains a success rate of 89.7% for pronominal references, working with English technical manuals. 	  ['Method_Citation','Results_Citation'] 
 They use limited knowledge (lexical, morphological and syntacticinformation sources) for the detection of the cor rect antecedent.These proposals have report high success rates for English (89.7%) (Mitkov, 1998) 	  ['Method_Citation','Results_Citation'] 
 However, the difficulty of our task can be verified according to the baseline experiment results reported in (Mitkov, 1998).Resolving pro­ nouns in English technical manuals to the most re­ cent candidate achieved a success rate of 62.5%, whereas in our experiments only 43.9% of the most recent candidates are resolved correctly as the an­ tecedent (cf. 	  ['Method_Citation','Results_Citation'] 
 The present work inherits the spirit of the supervised approaches to verb classification (Stevenson et al., 1999; Stevenson and Merlo, 1999; Merlo and Stevenson, 2001; Stevenson and Joanis, 2003; Joanis and Stevenson, 2003; Joanis et al., 2008). 	  Results_Citation 
 For example, Stevenson and Joanis (2003) report an accuracy of 29% (which implies mP U R ≤ 29%), but their task involves classifying 841 verbs to 14 classes based on differences in the predicate-argument structure. 	  Results_Citation 
 Following a strategy in line with work on verb classification (Merlo and Stevenson, 2001; Stevenson and Joanis, 2003), we set out to classify common noun lemmas based on their morphosyntactic distribution in a considerably larger corpus. 	 Results_Citation 
 Biemanns idea and motivation is that non- compositional expressions could be treated as single units in many NLP applications such as Information Retrieval (Acosta et al., 2011) or Machine Translation (Carpuat and Diab, 2010). 	 Result_Citation 
 For instance, Acosta et al. 	 Result_Citation 
 The results described in this section are based on the 18 confusion sets selected by Golding (1995; 1996). 	  Results_Citation 
 The memory-based learner was tested using the 18 confusion word sets from Golding (1995) on the WSJ section of the Penn Treebank and the Brown Corpus. 	  Results_Citation 
 Golding [3] proposed a Bayesian hybrid method to take into account all available evidence, instead of only the strongest one.The method was applied to the task of context-sentitive spelling correction and was reported to be superior to decision lists. 	  ['Method_Citation','Results_Citation'] 
 All methods use either the full set or a subset of 18 confusion sets originally gathered by Golding (1995). 	  Results_Citation 
 Most methods are trained and tested on Model Alta BNC Model Alta BNC f (t ) 72.98 70.00 f (w1 , t , w2 )/ f (t ) 87.77 76.33 f (w1 , t ) 84.40 83.02 f (w1 , w2 , t )/ f (t ) 86.27 74.47 f (t , w1 ) 84.89 82.74 f (t , w2 , w2 )/ f (t ) 84.94 74.23 f (w1 , t , w2 ) 89.24#*77.13 f (w1 , t , w2 )/ f (w1 , t ) 80.70 73.69 f (t , w1 , w2 ) 84.68 75.08 f (w1 , w2 , t )/ f (w2 , t ) 72.11 69.28 f (w1 , t )/ f (t ) 82.81 77.84 f (t , w1 , w2 )/ f (t , w1 ) 75.65 72.57 f (t , w1 )/ f (t ) 77.49 80.71# Table 5	  ['Aim_Citation','Results_Citation'] 
 Table 6 shows 3 An exception is Golding (1995), who uses the entire Brown corpus for training (1M words) and 3/4 of the Wall Street Journal corpus (Marcus et al., 1993) for testing. 	  Results_Citation 
 Feature-based approaches, such as Bayesian clasÂ­ sifiers (Gale, Church, and Yarowsky, 1993), deciÂ­ sion lists (Yarowsky, 1994), and Bayesian hybrids (Golding, 1995), have had varying degrees of sucÂ­ cess for the problem of context-sensitive spelling correction. 	  ['Method_Citation','Results_Citation'] 
 We consider an alternative method, Bayes, a Bayesian hybrid method (Golding, 1995), for the case where the words have the same part of speech. 	  Results_Citation 
 For CSSC, we tested our system on the identical data from the Brown corpus used by Golding (1995) 	  ['Aim_Citation','Results_Citation'] 
 The third main result was that we found very little agreement between our sub­ jects on identifying briding descriptions	  Results_Citation 
 For example, the best F-score in the shared task of BioNER in COLING 2004 JNLPBA (Kim et al., 2004) was 72.55% (Zhou and Su, 2004) 1, whereas the best performance at MUC6, in which systems tried to identify general named entities such as person or organization names, was an accuracy of 95% (Sundheim, 1995). 	  Results_Citation 
 The test corpus consists of 100 Wall Street Jour­nal documents from the period January 1993 to June 1994, 54 of which contained management succession events (Sundheim, 1995). 	  Results_Citation 
 It is not clear what resources are required to adapt systems to new languages."It is important to mention that the F-measure for the human performance on this task is about 96%, (Sundheim 1995). 	  Results_Citation 
